Modular AI Agent Ecosystem for Structured Software Development

Design Goal: Create a multi-agent system using LangGraph + LangChain that guides a software project through all stages of development. Each agent specializes in a phase of the SDLC (vision, architecture, tasks, development, review, deployment, feedback), with a central orchestrator managing the workflow. The system must interface with users via the Model-Client Protocol (MCP) (e.g. Claude Desktop, OpenWebUI) and integrate with GitHub for project artifacts and code. All documentation outputs will be stored as Markdown in the project’s docs/ directory.

Interaction & Operation via MCP

The ecosystem is exposed through an MCP server so that MCP-compatible clients (Claude Desktop App, OpenWebUI) can use it as if it were a single AI model. This MCP interface acts as the conversation gateway to the agent system. From the user’s perspective, they interact with one unified assistant, but behind the scenes the MCP server routes requests to the LangGraph orchestrator and its agents.
	•	Conversational Collaboration: In interactive mode, the user can converse naturally. The orchestrator interprets user commands or questions and delegates to the appropriate agent. For example, if the user says “List the current user stories”, the orchestrator will invoke the Task Refinement Agent to summarize the backlog. The user can also explicitly request a phase (e.g. “Run the Architecture design now”).
	•	Autonomous Workflow: The system also supports full autonomous execution. The user (or an external trigger, such as a new feature request issue) can initiate a pipeline run with minimal input (e.g. a project idea or specification). The orchestrator will then automatically step through all agents in sequence, from Vision & Scope through Deployment and Feedback, without further user intervention. Checkpoints can be built in (e.g. waiting for user approval after architecture is drafted) if a semi-autonomous mode is desired.
	•	MCP Integration: The MCP server ensures the streaming responses and tool outputs from the agents are communicated back to the client in real-time. It adheres to MCP message formats so that Claude or OpenWebUI can handle the multi-turn dialogue. For instance, when an agent needs to perform a tool action (like reading a file or running tests), the MCP server may send an <assistant> message with a special tag (if required by MCP) indicating a tool usage, ensuring the client can handle it. In general, the MCP interface abstracts the complex agent ecosystem behind a familiar chat interface.

LangGraph Orchestration & System Architecture

LangGraph is used to implement the orchestrated workflow of agents. Each stage-specific agent is modeled as a node in a directed graph, and edges define the flow of control and data between them. The orchestrator (managed by LangGraph) coordinates agent execution according to this graph. This design follows the principle that multi-agent workflows map naturally to a graph or state machine ￼.

Workflow of the multi-agent ecosystem through development stages. Each agent (node) produces artifacts that flow to the next. The dashed arrow indicates feedback from the last stage back to earlier stages to address issues or architecture drift.

Key aspects of the architecture:
	•	Agent Nodes: Each agent is an independent node with its own prompt and logic. They operate sequentially (with some potential branching for multiple tasks) as depicted above. The LangGraph orchestrator triggers them in order: Vision → Architecture → Task Refinement → Development → Review → Deployment → Feedback. The connections (edges) are configured so that the output of one agent becomes input context for the next. LangGraph manages these transitions and can conditionally loop or branch if needed (for example, iterating over multiple tasks).
	•	Shared State & Memory: Agents communicate by writing their results to the graph’s state (a shared memory context) which subsequent agents can read ￼. In practice, this is backed by a semantic vector store (Postgres + pgvector) to allow flexible queries. After an agent finishes (e.g. Architecture Agent produces an architecture doc), the orchestrator stores that document’s content and embedding in the vector database under a tag (e.g. “ArchitectureDoc”). The next agent (Task Refinement) can then query the vector store for relevant context (e.g. embedding search for “project goals” or “architectural constraints”) to inform its prompt. This shared memory ensures no agent works in isolation; all important knowledge (requirements, decisions, code summaries, past discussions, etc.) is globally available. It acts as an evolving knowledge base of the project.
	•	Control Flow: The orchestrator uses LangGraph’s ability to define conditional transitions and loops. For example, the Development Agent might be invoked once per task user story (looping through a list generated by the Task agent). The Review Agent might have a conditional loop where if it finds issues with the code, it could either auto-fix them (by invoking the Development Agent again for fixes) or mark the task as needing changes. Similarly, the Feedback Agent, after generating a retrospective, might have an edge looping back to the Architecture Agent (dashed arrow in the diagram) if significant architecture drift is detected that warrants re-design. In LangGraph terms, these can be implemented as state machine transitions or router nodes that decide the next step based on agent outputs (e.g. a router node checks if Review found “tests failed” and if so, route back to Development for bug-fixing).
	•	Modularity: Each agent node is encapsulated with a clear interface (inputs and outputs), allowing the workflow to be extended or modified easily. For instance, one could insert a “Security Analysis Agent” after Development and before Review without affecting other parts. This modular design aligns with best practices for multi-agent systems, making it easier to maintain and improve each component independently ￼. Each agent can even use a different LLM or model fine-tuned for its task if needed (LangGraph allows specifying different models/prompts per node) ￼.
	•	Observability: All agent actions and LLM calls are instrumented with Langfuse for observability. LangGraph’s integration with LangChain callbacks allows capturing each step in a trace ￼. We attach a Langfuse callback handler to the orchestrator – as each agent runs, events (prompts, responses, tool calls) are sent to Langfuse. This provides a rich trace of the workflow, enabling developers to debug issues (e.g., see why the Architecture Agent produced a certain design) and to analyze agent performance over time (e.g., how often the Review Agent finds problems). We also log key metrics: time per agent, number of iterations in dev loop, any errors/exceptions, etc., all viewable in the Langfuse dashboard. This observability is crucial for a long-running, autonomous agent pipeline in production.

In summary, the LangGraph-based architecture provides a controlled orchestration of specialized agents, where the graph state holds the cumulative project knowledge and flows between agents, and where we have full visibility into the system’s reasoning steps. Multi-agent design helps break down the complex software development process into tractable, focused units of work for each agent, improving reliability and maintainability ￼.

Stage-Specific Agents and Roles

Each stage of the development workflow is handled by a dedicated agent with expertise in that area. These agents embody software engineering best practices (Agile methodologies, TDD/BDD, etc.) in their prompt design and behavior. Below we describe each agent’s role, inputs/outputs, and prompting strategy:

1. Vision & Scope Agent

Role: Acts as the Product Manager/Analyst, establishing the project’s vision, goals, and scope. This agent’s job is to ensure the team starts with a clear understanding of requirements and context.
	•	Inputs: Initial project description, feature idea or high-level requirements. This could come from a user prompt, a GitHub issue (e.g. labeled “feature request”), or an existing PRD. The agent also retrieves any relevant context (via vector memory) such as similar past projects or domain knowledge.
	•	Behavior: The agent uses structured brainstorming to produce a Vision & Scope Document. It may follow a template like:
	•	Project Vision: High-level objectives and the “why” of the project.
	•	Key Features & Scope: What capabilities will the software include? List of high-level features or user needs.
	•	Out of Scope: To enforce focus, explicitly note aspects not being addressed.
	•	Stakeholders & Users: Who will use or benefit from the software.
	•	Constraints & Assumptions: Any technological, business, or timeline constraints, plus assumptions made.
	•	Success Metrics: (Optional) How we measure success.
The agent ensures all details needed to start development are present – essentially Definition of Ready for the project. For example, it will verify that the project goals, target users, and major requirements are clearly identified before moving on. (If something is vague, in an interactive setting the agent might ask the user clarifying questions; in autonomous mode it may log a warning or make reasonable assumptions.)
	•	Prompt Design: The prompt instructs the agent to act as an experienced product manager or business analyst. It emphasizes clarity and completeness:
	•	It may include guidelines from agile best practices, e.g. INVEST criteria for good user stories (Independent, Negotiable, Valuable, Estimable, Small, Testable) – though INVEST is for user stories, the Vision agent can still aim for these principles when listing features.
	•	It incorporates Definition of Ready (DoR) checkpoints to ensure the output is actionable. For example, before finalizing, the agent internally checks: “Are the project goals clear and agreed? Are the requirements feasible and testable? Do we have acceptance criteria in mind?” ￼. If not, it will refine the scope or highlight open questions.
	•	Few-shot example: The prompt might provide an example vision document for a sample project to illustrate the style and level of detail expected.
	•	Output: A Markdown document (e.g. Vision_and_Scope.md in docs/) capturing all the above sections. This document will be committed to the repository. It serves as the foundational reference for all other agents. The orchestrator saves this output to the vector store with high priority, since it contains core information to be reused. The Vision Agent might also open GitHub issues for each high-level feature identified, to seed the backlog (although detailed tasks are for the Task agent). If it does, it uses the GitHub integration to create those issues (with labels like “story” or “epic” as appropriate).

2. Architecture Agent

Role: Takes the Vision & requirements and produces the system architecture design. This agent is akin to a Solution Architect – responsible for the high-level technical plan. It ensures technical decisions are made explicit via ADRs and the system’s structure is well-defined before coding begins.
	•	Inputs: The Vision & Scope document (from the previous agent) is the primary input. The agent also looks at non-functional requirements or constraints from vision (e.g. “must handle 10k req/sec” or “use AWS serverless”). If the project is an extension of an existing codebase, the agent will inspect the repository (using the GitHub tools) to understand the current architecture. It may read architecture docs of previous projects or relevant modules via vector search for inspiration or reference.
	•	Behavior: The agent creates an Architecture Design Document and/or Architecture Decision Records (ADRs):
	•	It might break the architecture doc into sections: Overview/Context, Architectural Constraints, System Components & Responsibilities, Data Model, Integration Points, Technology Stack Choices, Risks and Mitigations.
	•	It will enumerate key architecture decisions (e.g. “Monolith vs Microservices”, “Database choice”, “Use of message queue”, etc.) and provide rationale for each decision. Using the ADR format for each significant decision is encouraged (ADR: decision, context, options, consequences) ￼. For example, an ADR might be saved in docs/architecture/ADR-use_postgres.md describing why PostgreSQL was chosen over MongoDB.
	•	The agent may sketch module structures or relationships. If needed, it can output simple diagrams using text (like ASCII art or Mermaid) to illustrate high-level architecture (e.g. component diagrams). However, since we prefer Markdown outputs, it might include a Mermaid code block for architecture diagrams that can render on GitHub.
	•	Architecture standards: The prompt reminds the agent to follow well-known design principles (e.g. SOLID, high cohesion, low coupling) and to consider Architecture Recovery Definition (ARD) standards. (Here “ARD” likely refers to ensuring the architecture description covers Requirements, architecture Rationale, and Design details clearly). It should also consider Definition of Done for architecture: e.g., architecture is “done” when it addresses all requirements and is validated against constraints.
	•	The agent will check for consistency with Vision: every major requirement from the Vision should be mapped to some component or design decision in the architecture. If something cannot be accommodated, it flags it.
	•	Prompt Design: The Architecture Agent’s prompt positions it as “a senior software architect following proven practices”. It might include guidelines such as:
	•	“Produce an ADR for each significant decision, including context and consequences, so future developers understand why the decision was made ￼.”
	•	“Use diagrams or bullet lists to outline the system’s components and how they interact. Mention patterns (MVC, microservice, layered architecture, etc.) if relevant.”
	•	The prompt may also instruct: “Ensure the design is language-agnostic unless implementation specifics are necessary. Focus on concepts (e.g. REST API design, database schema) rather than code, so the design applies to Python or Go alike.”
	•	If the project language or runtime is known (say the repository has a pyproject.toml or go.mod), the agent will incorporate language-specific considerations (for example, for Python, deciding on a FastAPI framework, for Go, perhaps an HTTP router library). However, these specifics are always clearly justified and not assumed blindly.
	•	Output: A Markdown architecture document (e.g. Architecture.md in docs/) plus any ADR files as needed. The main Architecture.md will summarize the overall design and reference individual ADR files for detailed decisions. All these files are created/updated in the repo via GitHub API. The orchestrator logs these outputs and updates the semantic memory (embedding the text for later stages). The architecture doc will be used directly by developers and for verifying consistency during the Feedback stage (to catch architecture drift).

3. Task Refinement Agent

Role: This agent acts as the Project Planner or Agile Coach, taking the big-picture plan and breaking it down into a detailed backlog of tasks. Essentially, it performs story slicing and writes acceptance criteria (following BDD style) for each task. It ensures each task meets Definition of Ready (clear, actionable, testable) before development starts.
	•	Inputs: The Vision & Scope and Architecture documents are its primary context. From the vision it gets the high-level features/goals; from architecture it understands the components that need building. It may also ingest any existing GitHub issues labeled as user stories or tasks (via the GitHub toolkit’s “Get Issues” tool). If the Vision Agent already created some epic or story placeholders, the Task Agent refines those.
	•	Behavior: The output is a set of user stories or development tasks, each with well-defined acceptance criteria:
	•	The agent could create a Backlog Markdown file (docs/Backlog.md or UserStories.md) listing all user stories. Each story might be formatted as:
	•	User Story: “As a [user], I want [some capability] so that [benefit]…” (standard agile story format).
	•	Acceptance Criteria: A bullet list of conditions that must be met for the story to be done. Ideally written in BDD scenario format:
	•	Given [initial context], when [event], then [outcome]. For example: “Scenario: User submits an empty form. Given the user is on the signup page, when they submit without filling fields, then an error message should prompt for required info.”
	•	DoR Checklist: (implicitly, the agent ensures each story has the attributes of INVEST and meets DoR). Stories should be independent and small enough to complete in one iteration ￼ ￼. If a story is too large or dependent, the agent will split or rearrange it. It ensures each story is testable, i.e., it can formulate clear tests for it ￼.
	•	The agent prioritizes tasks if needed (it can assign story points or a priority order).
	•	If using GitHub issues to track tasks, the agent can also create an issue for each user story with the description and acceptance criteria. However, since final docs must be in docs/, it will also compile them into the Markdown backlog file for permanence in the repo.
	•	Definition of Ready application: The agent explicitly cross-checks that each story has all info needed for a developer. For example, it will incorporate any specific technical requirements from architecture into the relevant story’s details. If a story is “Implement authentication service”, the acceptance criteria will note which authentication method, any compliance requirements, etc., drawn from the architecture doc. Each story effectively becomes a mini-spec that a developer can pick up and know what to do. Stories not meeting these criteria would be revised by this agent until they do (or flagged for clarification).
	•	Prompt Design: The prompt defines the agent as “an Agile Scrum Master breaking down requirements.” It instructs:
	•	Use BDD syntax for acceptance criteria whenever possible, to ensure testable outcomes (Given/When/Then format).
	•	Adhere to INVEST principles: each story should be independent, negotiable, valuable, estimable, small, and testable ￼ ￼.
	•	Ensure each story’s Definition of Ready: the story is clear, feasible, and agreed upon, with understanding of “what does done look like” (which is basically the acceptance criteria) ￼ ￼.
	•	The prompt might include an example of a well-written user story with acceptance tests to guide the format.
	•	Output: A markdown file (or set of files) listing the refined tasks. For example:

## User Story 1: User Registration
**As a** new user, **I want** to register an account **so that** I can access member-only features.

**Acceptance Criteria:**
- Given I am a visitor on the site, when I fill the registration form with valid info and submit, then my account is created and I receive a confirmation email.
- Given ... (additional scenarios)

## User Story 2: ...

This file (docs/UserStories.md) is committed via GitHub. Additionally, the agent could update GitHub Issues: for each story, either create a new issue or update an existing one with the story description and a checklist of acceptance criteria. Because it has write access to issues, it can also label them (e.g. label “ready” when done refining). Using the GitHub API tools, it might call Create Issue and Comment on Issue for this purpose ￼. All newly created issues and the backlog doc are linked (the doc can list issue IDs, and issue descriptions can mention the doc).
The backlog now serves as the plan that the Development agent will iterate through. The orchestrator marks each story as ready for development (per DoR, all questions answered) and proceeds.

4. Development Agent (TDD/BDD Focused)

Role: This agent plays the Developer role, implementing the code for each task in a test-driven manner. It works story by story, writing tests (or specifications) first, then code to satisfy them, ensuring Behavior-Driven Development and high code quality. It interacts heavily with the codebase via GitHub – reading/writing files, running tests, and making commits.
	•	Inputs: A specific user story or task from the backlog, including its acceptance criteria. It also takes into account the architecture (so it knows where in the design this code fits) and existing codebase. Before coding, the agent will retrieve relevant code files (using GitHub tools like “Read File” or search code ￼) to understand the context (e.g., if adding a new module, see how similar modules are structured; if modifying, get the file content).
	•	Behavior (per Task): For each task, the Development Agent follows a TDD cycle:
	1.	Generate Test (Spec): It writes a unit test or an acceptance test corresponding to the story’s criteria. For example, if using Python, it might create a tests/test_featureX.py file with pytest functions for each scenario; if Go, perhaps a _test.go file. If the story is high-level, it may create an integration test or behavior test (possibly using a BDD framework like Behave or Cucumber syntax in a .feature file). These tests initially fail (since code isn’t implemented yet).
	2.	Run Tests (mentally or via tool): Optionally, the agent could actually run the tests by invoking a test runner tool. With MCP integration, executing code might be sandboxed, but if the environment allows, it could use a tool to run pytest or go test. If using a safe environment like the Claude app’s sandbox, it might simulate this. Assuming it cannot actually execute in this context, the agent at least “imagines” the test results or relies on later CI feedback. (In practice, integration with an actual CI pipeline can run tests for real when the code is pushed – see Deployment agent.)
	3.	Write Code to Pass Test: The agent then writes the minimal code needed to implement the feature and make the test pass. It creates new source files or edits existing ones accordingly. This code generation uses best practices: clear naming, adherence to the architecture (e.g., follow designated layers or patterns), and includes inline docstrings or comments as needed.
	4.	Refactor: After passing tests (conceptually), it ensures the code is clean. It may adjust structure for readability or performance while keeping tests green.
	5.	Repeat for next test or next story: It may write additional tests for edge cases and implement those, until the acceptance criteria of the story are fully satisfied.
	•	Prompt Design: The Development Agent’s prompt is critical. It is instructed to “act as a conscientious software engineer practicing TDD.” Some prompt elements:
	•	Behavior-Driven cues: It might be given the acceptance criteria in Gherkin format and told: “Write code such that these scenarios pass. First, translate each scenario into a test.”
	•	Emphasis on small iterations: Write one test, implement just enough, then proceed – this can be hard to enforce without actual code execution, but the agent can simulate the logical steps.
	•	Include Definition of Done (DoD) criteria for development: not just writing code, but also documentation and passing tests. For example, remind that done = code is committed, tests are written and all pass, code is reviewed, and documentation (like docstrings or user docs if needed) updated ￼.
	•	The agent is told to avoid language-specific pitfalls and follow style guidelines (PEP8 for Python, effective Go idioms for Go, etc.), which it can glean from context or an initial style prompt.
	•	Possibly incorporate a few-shot example: a simple function where it first shows a test then code.
	•	Tools & GitHub Integration: This agent uses the GitHub toolkit extensively:
	•	Read files: to get context (e.g., Read File tool to open an existing module).
	•	Create/Edit files: It uses Create File for new files, or Update File for modifications ￼. The content is the code it generated. For each code file changed, it prepares a commit message summarizing the change (including the story reference).
	•	Commit & Branching: The agent might operate on a feature branch (the orchestrator or a prior setup can use Create a new branch and Set active branch tools ￼ so that development is isolated). Each story could be a branch or all stories in one branch – depending on the workflow. It’s reasonable each user story = one branch & PR, to mirror typical GitFlow. The agent can commit multiple times during development (e.g., one commit for adding tests, one for implementation, etc.), or a single squashed commit. Commit messages are generated to be clear (e.g., “Add user registration feature with tests (resolves #IssueNumber)”).
	•	Running code/tests: If execution is allowed via MCP (some setups allow running code in a safe environment), the agent could run tests. If not, this step might be simulated or deferred to CI. The agent might still include a “Run Tests” tool action that in a real deployment triggers the test runner, and the result (fail/pass) could come back for the agent to react (fail = write more code).
	•	Output: The tangible outputs are code changes in the repository. The agent ensures that for each story, the acceptance tests are added and the code passes them. It also might update documentation in docs/ if the feature needs user-facing docs or if a README needs updating (adhering to DoD that docs are updated ￼). For example, after implementing an API, update docs/API.md or code comments. All these changes are committed.
	•	Additionally, the Development Agent might produce a brief Test Report or Dev Log in Markdown for the story (perhaps in the commit message or a separate docs/test_results.md). However, since CI will handle test running, this may be unnecessary. Instead, it could log results to the console output.
	•	When a story is fully implemented, the orchestrator moves to the next story. Once all planned tasks for the iteration are done (or a milestone reached), it triggers the Review Agent by opening a Pull Request containing all the commits.

5. Review (PR & Code Quality) Agent

Role: Functions as a Code Reviewer/QA Engineer. It reviews the code changes (either as a Pull Request diff or a set of commits) to ensure quality, correctness, and completeness. It verifies that all Definition of Done criteria are met before code is merged.
	•	Inputs: The pull request diff or list of commits from the Development stage. The agent will use GitHub tools to fetch the PR (Get Pull Request), list files changed (List Pull Requests' Files), and read diffs or file content as needed ￼ ￼. It also takes in the context of the story and acceptance criteria (from the task description) to validate that the changes indeed fulfill the requirements.
	•	Behavior: The Review Agent performs a multi-faceted review:
	•	Verify Acceptance Criteria: It cross-checks each acceptance criterion from the user story against the implemented tests and code. If a criterion was “password must be at least 8 chars”, it looks in the tests for that scenario and in code for that logic. If anything is missing, that’s a review finding.
	•	Code Quality and Style: It analyzes the code for clarity, maintainability, and adherence to project style guidelines. For example, if Python, check for PEP8 compliance or potential performance issues; if Go, ensure idiomatic usage, etc. It may comment on long functions, lack of error handling, etc. (Since it’s an AI, it does static analysis reasoning – it doesn’t run linters unless integrated.)
	•	Testing Adequacy: Ensures tests cover happy path and edge cases. If tests are lacking scenarios, it will note that. Possibly it could generate additional tests if major gaps are found.
	•	Documentation and Comments: Check that any user-facing docs are updated if needed, and code is commented if non-obvious. This ties to DoD: e.g., “Documentation updated” is a criterion ￼.
	•	Definition of Done Checklist: The agent essentially goes through a DoD list:
	•	All unit/integration tests passed (it might rely on CI results for this; if CI indicates failures, the agent definitely flags it).
	•	Code is fully reviewed (that’s what it’s doing).
	•	No TODOs or obvious bugs remain.
	•	The feature meets acceptance criteria and is integrated with the rest of the system.
	•	The code has no obvious security or performance issues for this scope.
	•	If any criterion is not met, it marks the PR as needing changes.
	•	Security/Performance (if applicable): If within scope, the agent can do a basic security scan of the changes (e.g., check for use of vulnerable functions, proper input validation) and performance considerations (like not using an O(n^3) algorithm unintentionally). For more in-depth analysis, a specialized “Security Agent” could be inserted, but here the Review agent can handle basic checks.
	•	Prompt Design: The prompt casts the agent as “an expert software engineer reviewing a colleague’s pull request.” It instructs to be thorough but constructive. Key guidelines:
	•	Use a checklist of common review points: functionality, tests, readability, complexity, adherence to requirements.
	•	Incorporate Definition of Done explicitly: e.g., “Ensure all conditions for ‘done’ are satisfied: code reviewed, all tests pass, documentation updated, deployment scripts (if any) ready ￼.”
	•	If issues are found, the agent should either fix them (if trivial and within its toolset) or leave comments for the developer. Since we have autonomous capability, the Review agent can actually attempt minor fixes: it could itself use the GitHub file update tool to make small changes (like correcting a typo or adding a missing test) and push a commit to the PR branch. For larger issues, it can provide feedback.
	•	Possibly format output as a PR review commentary, e.g. a list of remarks or using the GitHub API to leave review comments (Create review request or similar tool).
	•	Output: If the code is solid (passes DoD), the Review Agent approves the Pull Request. This could be manifest as:
	•	Posting a PR review comment that “LGTM” (Looks Good To Me) or marking approval via GitHub API.
	•	Merging the PR: The agent can use Merge Pull Request (if such a tool exists, or use GitHubAPI to merge). Alternatively, this could be left to an automated rule or the Deployment Agent.
If issues are found, the output could be:
	•	A Review Report in Markdown (saved to docs/ or just as PR comments) itemizing the issues to address. For instance, docs/CodeReview_Story1.md listing feedback.
	•	The agent might also create new GitHub issues for follow-ups (e.g., “Tech Debt: Refactor login module” if something is acceptable for now but should be improved later).
	•	The orchestrator will then either loop back to the Development Agent to fix these issues or wait for a human developer to intervene, depending on settings. In fully autonomous mode, trivial fixes can be auto-corrected by the AI (the Review agent itself or by spawning a new Dev cycle for fixes). In collaborative mode, it might hand off to the user to make changes or confirm how to proceed.

6. Deployment (CI/CD) Agent

Role: Acts as the DevOps Engineer or CI/CD pipeline. This agent’s purpose is to ensure the newly merged code is successfully deployed or ready for deployment. It integrates with CI/CD processes, updates deployment scripts, and monitors the pipeline.
	•	Inputs: The now-approved code on the main branch (or a release branch). Input could come in the form of a successful merge event. The agent can query the repo for CI config files (like .github/workflows for GitHub Actions, or others) to see if pipelines exist. It also looks at infrastructure as code or deployment docs in the repo.
	•	Behavior: Depending on context, this agent may:
	•	CI Pipeline Configuration: Ensure that a CI pipeline is in place to build/test/deploy the project. If the repo lacks a pipeline, the agent can create one (for example, writing a GitHub Actions workflow YAML in .github/workflows/ci.yml). It would configure it to install dependencies, run tests, and perhaps deploy to a staging environment. This config is committed to the repo.
	•	Deployment Scripts: If the project requires Docker, Kubernetes manifests, or other deploy artifacts, ensure they are present or updated. The agent may create a Dockerfile or Helm chart based on project needs, or update a deployment doc with instructions.
	•	Versioning: It might bump version numbers if a release is being prepared, following semantic versioning based on the scope of changes (this could be automated by reading commit messages or PR labels).
	•	Trigger Deployment: If the pipeline is automated, merging the PR may already trigger a deployment (for example, CI runs tests and deploys to a test environment). The agent monitors this. Using the GitHub API, it can check commit status or CI results (“Commit statuses (read)” permission ￼ allows reading if CI passed). If deployment fails, it will alert or create an issue.
	•	Integration with MCP: The agent could report back in the chat something like “Deployment pipeline executed, all tests passed, app deployed to staging.” If errors occurred, it reports failure logs.
	•	In some configurations, this agent could actually be the CI runner (though typically, one would rely on external CI services). More realistically, the agent ensures everything is set up for CI/CD and perhaps kicks off a deployment through an API call or by pushing a special commit/tag.
	•	Prompt Design: The agent is prompted as “an experienced DevOps engineer”. Guidelines:
	•	Follow Infrastructure-as-code best practices: any change to deployment config is made through repository files (so it’s reproducible).
	•	Ensure security in CI (e.g., no plaintext secrets – possibly instruct to use GitHub secrets for API keys).
	•	The prompt might have examples of simple CI YAMLs or Dockerfile creation if needed.
	•	Emphasize being language-agnostic: e.g., if Python project, maybe set up a Python testing action; if Go, use Go’s tooling. The agent should detect project language (from files or config) and apply appropriate actions.
	•	Output: The agent will output any deployment-related artifacts to the repo:
	•	E.g. .github/workflows/ci.yaml (committed via Create File), Dockerfile, docker-compose.yaml, or a docs/Deployment.md with instructions.
	•	If the deployment is manual or the project is a library, Deployment.md can outline how to release the build.
	•	It will also provide a summary in the conversation (if interactive) about the deployment status (like a release note).
At this point, the software should be built, tested, and (optionally) deployed to an environment. The project’s pipeline is in place for future commits as well.

7. Feedback Loop Agent

Role: Represents the Product Owner or Retrospective Facilitator. This agent closes the loop by analyzing the outcome of the development cycle and preparing feedback: it might gather user feedback, evaluate if the initial vision was met, and detect any architecture or process drifts for the next iteration. Essentially, it conducts a “retrospective” and updates documentation accordingly.
	•	Inputs: Various sources:
	•	Post-deployment feedback: It could read newly opened GitHub issues that might represent bug reports or user feedback after deployment (using Search issues or filtering by labels like “bug” ￼).
	•	Runtime metrics or logs: If the system produces logs or if monitoring data is accessible (this is outside GitHub, but perhaps a future integration could feed performance metrics to the agent).
	•	Comparison of Artifacts: The agent retrieves the Vision & Scope and Architecture docs from memory and the final codebase structure from GitHub (e.g., list of modules, actual architecture). It compares the planned vs actual architecture to detect architectural drift – for example, did the team end up implementing additional modules not in the design? Or was a prescribed design pattern not followed? (It may do this via static analysis: e.g., architecture doc said “3 microservices” but the codebase ended up with one monolith – the agent would flag that).
	•	Team process reflections: If there were any deviations (e.g., many review fixes needed, tests that were hard to write), the agent can note process improvements. This info might come from logs (like if we track how many times we had to loop between Dev and Review).
	•	Behavior: The agent produces a Retrospective Report (Markdown) and possibly updates the architecture docs:
	•	Retrospective Report: Saved as docs/Retrospective_<iteration>.md (or appended to a single docs/Retrospective.md). It follows an Agile retro format: What went well, what didn’t, and action items.
	•	Went well: e.g., “All user stories were implemented on time”, “CI pipeline caught 2 failing tests which were fixed quickly”, “No major bugs reported in initial testing”.
	•	Needs improvement: e.g., “Story 5 lacked clarity initially, causing a delay”, “Architecture planned for separate services, but due to time we implemented a monolith – risk of architecture drift noted”.
	•	Action Items: e.g., “Next cycle, involve security review earlier”, “Refactor auth module as per original microservice plan”, “Improve test coverage for payment module”.
	•	Architecture Drift Analysis: If the agent finds drift, it documents it. For example: “The implemented architecture deviated from design: original design called for a separate auth service, but it was implemented as library in the monolith ￼. Consider revisiting this in the next iteration.” The agent might update the Architecture.md or ADRs with notes (or create a new ADR if the deviation was an intentional decision change).
	•	Feedback to Vision: If user feedback indicates a change in requirements or scope creep, the agent may suggest updates to the Vision & Scope doc. It could even modify that doc to reflect new understandings (effectively starting the next cycle’s planning with updated vision).
	•	Prompt Design: The agent is instructed to be “analytical and objective in assessing the project.” It uses known methodologies:
	•	“Apply the Agile retrospective format (Start/Stop/Continue or Good/Bad/Improve).”
	•	“Identify any architectural technical debt or drift – where does the current system diverge from the planned architecture and why ￼? Propose remedies.”
	•	The prompt might include that it should not blame, just state facts and improvements (since it’s essentially talking to the team).
	•	Encourage referencing concrete evidence: if a particular issue was opened or a test failed, mention it. (The agent can pull issue titles or test names via tools to include in the report.)
	•	Output: The Retrospective Markdown document with findings and next steps. It’s committed to docs/. Additionally, the agent might open GitHub issues for each action item (for example, an issue “Refactor auth into service [architecture improvement]” or “Investigate load performance for feature X”). This ensures the next cycle’s backlog includes these improvement tasks. The agent might label them as “tech debt” or “enhancement”.
The Feedback Agent’s outputs feed into the next iteration. The orchestrator can loop back to the Vision & Scope Agent (dashed arrow in the diagram) if a new cycle begins, using the updated context. This closes the SDLC loop and enables continuous improvement.

Prompt Templates and Best Practices

Each agent uses a robust prompt template embodying its role. We ensure prompts are reusable and standardized, incorporating industry best practices (TDD, BDD, DoR/DoD, ADR frameworks) so that the agents produce consistent, high-quality outputs. Below is an outline of the prompt structure for each agent:
	•	Vision & Scope Agent Prompt:
	•	Role: “You are a Product Manager and Business Analyst.”
	•	Goal: “Draft a clear Vision & Scope document for the project.”
	•	Content Guidelines: Include sections (Vision, Goals, Features, Non-Goals, Constraints, etc.). Ensure all requirements are understood (Definition of Ready checklist: target users, goals, tasks value, feasibility, timeline, definition of done criteria known ￼).
	•	Style: High-level, concise, avoid technical jargon, focus on what and why.
	•	Reusability: This prompt can be applied to any project domain; it avoids domain-specific assumptions except using the input context.
	•	Architecture Agent Prompt:
	•	Role: “You are a Software Architect.”
	•	Goal: “Design the software architecture and record key decisions.”
	•	Content Guidelines: Use ADR format for decisions ￼, provide a system overview, component breakdown, and how requirements map to components. Follow any specific constraints (e.g., must use AWS).
	•	Include: “List at least X architecture decisions with context and decision outcome.” (The prompt ensures the agent explicitly enumerates decisions).
	•	Style: Technical but clear. Prefer diagrams or bullet lists to lengthy prose where appropriate.
	•	Few-shot Examples: Possibly include a short example of an ADR or a brief architecture outline from a similar project to guide structure.
	•	Task Refinement Agent Prompt:
	•	Role: “You are an Agile Scrum Master & Analyst.”
	•	Goal: “Derive user stories and acceptance criteria from the vision/architecture.”
	•	Content Guidelines: For each feature, write a user story (“As a …, I want …, so that …”) and BDD-style acceptance criteria. Ensure each story is INVEST and ready ￼ ￼. If a story is too large or unclear, split or clarify it.
	•	Style: Use markdown headings for each user story, bullet points for criteria. Keep each story description concise but specific.
	•	Checks: “Make sure every requirement from the Vision doc is covered by at least one story” (prompt the agent to verify completeness).
	•	Development Agent Prompt:
	•	Role: “You are a Software Developer practicing TDD.”
	•	Goal: “Implement the user story. First write tests for each acceptance criterion, then implement code to make tests pass.”
	•	Content Guidelines: Work in steps: 1) Output the test code (in a markdown code block perhaps or via create-file tool). 2) Then output the implementation code. Keep implementations aligned with the architecture (e.g., place code in the correct module).
	•	Constraints: “Do not assume functionality that isn’t specified; if an acceptance criterion is unclear, you may infer a reasonable behavior.” Also, “Ensure code is idiomatic for the project’s language and passes all tests.”
	•	Style: It might produce one file at a time. The orchestrator or the agent’s chain can ensure it writes tests first, then triggers test run, then proceeds (possibly requiring some function-calling chain rather than a single prompt – we could implement this agent as a mini-chain of its own: a node that calls an LLM to generate test, another to generate code, etc., orchestrated by a loop until tests pass).
	•	Example: Could include a simple scenario like: Acceptance: “given no input, function returns error” -> test -> code as a demonstration.
	•	Review Agent Prompt:
	•	Role: “You are a Senior Engineer reviewing a Pull Request.”
	•	Goal: “Review the code diff for quality and completeness.”
	•	Content Guidelines: Remind the agent to check functionality vs requirements, code style, test coverage, and DoD criteria ￼.
	•	Possibly provide it with a checklist in the prompt:
	1.	Requirement fulfillment?
	2.	All acceptance tests present and passing?
	3.	Code follows style and best practices?
	4.	No obvious bugs or security issues?
	5.	Documentation and comments updated?
	•	Style: If issues, respond with specific comments referencing line numbers or file names (the agent can output suggestions or use GitHub review comment format). If none, respond with an approval statement.
	•	Note: The prompt might include an example of a good code review comment versus a poor one, to guide tone (e.g., “Instead of just ‘bad code’, explain why and how to improve.”).
	•	Deployment Agent Prompt:
	•	Role: “You are a DevOps Engineer.”
	•	Goal: “Ensure the project is deployable and CI/CD is set up.”
	•	Content Guidelines: If CI pipeline config file is missing, create one. If present, verify it covers building, testing, and deploying. If environment config is needed (like adding secrets or cloud setup), mention it.
	•	Example Instructions: “If no CI found: generate a GitHub Actions workflow for {language} with steps: checkout, install, run tests, build artifact. If deploying to say Heroku/AWS, ensure config (this may rely on user input about environment – if not known, just prepare generic).”
	•	Post-merge: Possibly include steps to tag a release or any manual deployment verification needed.
	•	Style: YAML or shell scripts should be formatted correctly. Provide comments in any script to explain.
	•	The agent might not need a few-shot example because its output is very specific to context (but maybe a snippet of a generic CI yaml could help).
	•	Feedback Agent Prompt:
	•	Role: “You are a Project Lead analyzing the iteration retrospectively.”
	•	Goal: “Summarize what was learned, identify improvements for next time, and note any deviations between plan and outcome.”
	•	Content Guidelines: Use sections like:
	•	What went well,
	•	What can be improved,
	•	Action items.
	•	Architecture vs implementation: list any architecture drift or technical debt.
	•	User feedback: summarize any feedback or issues reported.
	•	Encourage constructive output: The agent should phrase things as “For next iteration, we should X” rather than just pointing blame.
	•	Style: Narrative form for the retrospective, bullet points for action items. Possibly tag items with owners if relevant (though here agents = team collectively).
	•	Could include an example of a short retrospective report for a dummy project to illustrate format.

These prompt templates are stored and version-controlled, making them easy to update as best practices evolve. The modularity of our design means each prompt can be refined independently – e.g., if we find the Development Agent isn’t writing sufficient tests, we can tweak its prompt to emphasize that, without affecting others.

GitHub Integration Details

The ecosystem heavily leverages GitHub for real-world grounding. LangChain’s GitHub toolkit is used to grant agents the ability to read and modify the repository. Each agent, as needed, is equipped with tools like:
	•	Get Issues / Search Issues: to retrieve user stories, bug reports, or feedback from GitHub issues ￼.
	•	Read File / Create File / Update File / Delete File: to inspect and modify repository files ￼. This is how agents write documentation to docs/ and how the Dev agent writes code.
	•	Create Pull Request / Get Pull Request / List PRs / Comment on Issue/PR: for managing PR workflow ￼. The Review agent uses these to fetch diffs and to possibly post review comments. The Deployment agent might use PR merge or comment triggers.
	•	Branch management tools: List Branches, Create Branch, Set active branch ￼ – used to isolate feature development and manage versions.
	•	Commit status and metadata: The toolkit ensures the agent has permissions to read commit statuses and PR checks ￼, so the Deployment agent can see CI results and the Review agent can see if tests passed.
	•	Environment: The GitHub API credentials (app ID, keys, repository name) are configured in the environment so that agents authenticate and operate with appropriate permissions. Each action the agent takes (like writing a file or creating an issue) goes through these tools, which in turn call GitHub’s API.

This integration means all agent-generated artifacts persist in the repository, satisfying the requirement that final documentation lives in docs/. For instance:
	•	Vision & Scope Agent commits docs/Vision_and_Scope.md.
	•	Architecture Agent commits docs/Architecture.md (and possibly a docs/architecture/ folder with ADRs or diagrams).
	•	Task Agent commits docs/UserStories.md (and/or creates issues reflecting them).
	•	Review Agent might update docs/CodeReview.md or simply use PR comments (which are recorded on GitHub).
	•	Deployment Agent might commit CI configs or produce docs/Deployment.md if needed for manual steps.
	•	Feedback Agent commits docs/Retrospective.md.

By using GitHub as the central knowledge repository, we ensure traceability and collaboration:
	•	Traceability: Every action is represented as a commit or issue, so humans can inspect what the AI did. For example, one can look at the Git history to see changes an agent made in an ADR file.
	•	Collaboration: Human team members can intervene at any time by pushing their own commits or comments. The agents will pick those up (since they read from GitHub). For example, if a human developer directly commits a change, the Review agent will include it in the PR review. If a human adds a new issue with feedback, the Feedback agent will consider it.

Safety mechanism: We likely run the agents with a separate service account or bot account on GitHub with limited permissions (only that repo). The commit messages and issue comments by the bot should indicate an AI agent did it, for transparency.

Finally, note that the agents avoid making language-specific assumptions in documentation:
	•	All docs are written in general terms (e.g., “the service will expose a REST API” rather than “we will use Flask” unless Flask was a conscious decision).
	•	Code generation obviously is language-specific, but it’s only done by the Dev agent with full context of the project’s language. If the project is in Go, the agent won’t produce Python code because it will see the repository context (like Go mod file). The prompt or system config will provide the primary language info.
	•	If a project is multi-language (rare, but e.g. a frontend in JS and backend in Python), the agents can handle that by generating appropriate files in each language’s context. The prompts and design encourage focusing on role responsibilities (which are language-agnostic at the high level) and using context to adapt to the language at implementation time.

Maintainability and Extensibility of the Ecosystem

Our modular agent architecture is designed for maintainability:
	•	Separation of Concerns: Each agent has a single, well-defined responsibility aligned with a development stage. This makes the system easier to reason about and debug, since you can isolate which phase might be causing an issue. It also maps well to human team roles, which is intuitive.
	•	Independent Iteration: Because agents communicate via shared state (documents, issues) rather than tightly coupling, we can improve one agent without breaking others. For example, if we find a better way to prompt the Architecture Agent to include threat modeling, we can update its prompt template alone. The overall workflow doesn’t change, as long as the output (architecture doc) remains in the expected format.
	•	Reusability: Common functionality (like accessing the vector store or parsing an issue description) can be factored into utility nodes or the orchestrator. LangGraph allows defining subgraphs or nodes that can be reused by multiple agents. For instance, a “Search knowledge base” node (querying pgvector) might be used in the prompts of Architecture, Task, and Feedback agents. If we update how our semantic search works (say switching embedding model), we update that in one place.
	•	Project Structure: We organize the code as suggested by best practices ￼ ￼:
	•	Each agent’s prompt and logic in its own module (e.g., agents/vision_agent.py, agents/architecture_agent.py, etc.).
	•	Shared utilities (GitHub client setup, vector DB client, common schemas for docs) in a base module.
	•	Workflows orchestrated in a central LangGraph definition (could be a YAML or Python definition of the graph). This workflow explicitly connects outputs to inputs.
	•	This clean separation makes it easy to add a new agent. For example, if later we want a Security Audit Agent, we add it in the graph after Review (or parallel to it), without overhauling existing agents.
	•	Observability & Debugging: With Langfuse and logging, if something goes wrong (e.g., the Development Agent produces syntactically incorrect code or a prompt leads to hallucination), we can trace the exact prompts and responses. The system can log each agent’s output to the console or a file as well, which during development of the AI pipeline is invaluable. We can even build automatic evaluations: e.g., after each stage, validate the output (markdown lint for docs, compile check for code). If validation fails, the orchestrator can catch it and either fix it (maybe by re-prompting with stricter instructions) or flag for human intervention.
	•	Scalability: The LangGraph framework and the stateless nature of each agent call means we could distribute agents if needed (for example, run the Dev agent on a powerful code-generation model, but the Vision agent on a smaller model). The graph could also potentially run agents in parallel when appropriate (though our pipeline is mostly sequential, some parts could be parallelized, e.g., multiple development tasks could be coded in parallel by multiple instances of the Dev agent if resources allow).
	•	Upgradability: Because prompts and logic are version-controlled, updating to new LLM versions or adding support for new languages is straightforward. For a new programming language, we might adjust the Development Agent prompt to include that language’s testing framework. The system is otherwise language-agnostic; there’s no hardcoded assumption that would break if we switch from Python to, say, Java. The agents derive what to do from context (e.g., reading a pom.xml would clue the Dev agent it’s a Java project and it would then write Java tests).
	•	Modularity in UI: Since the MCP interface presents one unified assistant, we can maintain a consistent user experience even as we modify the internal architecture. Users just see an AI that can handle a software project conversation. We can swap out how the sausage is made (agents behind the scenes, or even try a single large agent monolith if we wanted) without changing that interface.

In conclusion, this LangGraph-powered multi-agent ecosystem provides a structured, end-to-end solution for AI-assisted software development. It guides the process from initial idea to deployed software, with each agent contributing specialized expertise and producing tangible artifacts (code or docs) stored in the repository. The design ensures that the workflow is transparent (everything is in version control), collaborative (users or developers can interact at each stage via MCP), and continuous (the feedback loop allows learning and adapting in subsequent iterations). By leveraging well-defined prompts and strong engineering practices (BDD, TDD, ADR, agile workflows), the system aligns AI activities with proven software development standards, increasing the likelihood of meaningful and correct outcomes at each stage.

Ultimately, this modular agent architecture can accelerate development while maintaining quality, and it can be extended or customized to different project needs, programming languages, or organizational processes with minimal friction. By combining LangChain/LangGraph’s orchestration with GitHub’s collaboration platform, we get an AI agent team that works much like a real-world dev team, in a repeatable and manageable way.